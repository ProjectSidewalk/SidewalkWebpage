@import models.user.User
@(title: String, user: Option[User] = None)

@main(title) {
@navbar(user)
<div class="container">
    <h2 class="">Mission</h2>
    <div class="row">
        <div class="col-lg-12">
            <p class="bold">
                We are researchers at the University of Washington and Maryland focused on transforming how sidewalk accessibility data is collected and visualized using a combination of crowdsourcing and machine learning. And we need your help.
            </p>
        </div>
    </div>

    <h2 class="">What is Project Sidewalk?</h2>
    <div class="row">
        <div class="col-lg-12">
            <p class="bold">
                <a href="http://projectsidewalk.io">Project Sidewalk</a> is a new online tool that enables anyone—from motivated citizens to government workers—to virtually walk through cities to locate, label, and assess sidewalks. Unlike traditional sidewalk assessments, you do not need to leave your home to help--Project Sidewalk allows you to virtually explore cities from your desktop or laptop.

                User-contributed labels are used to develop new accessibility-friendly mapping tools (e.g., route planners, map visualizations), to train machine learning algorithms to semi-automatically assess cities in the future, and to create better transparency about city accessibility (imagine a <a href="http://walkscore.com">walkscore.com</a> for sidewalk accessibility!).
            </p>
            <p class="bold">
                You can help by completing a few missions (<a href="http://projectsidewalk.io/audit">click here to start!</a>) and by sharing about Project Sidewalk with your family, friends, and colleagues via email or social media (for example, you could <a href="https://twitter.com/projsidewalk/status/888040060163162117">retweet this tweet</a>!). You can also 'like' us on <a href="https://www.facebook.com/projsidewalk/">Facebook</a>!
            </p>
            <p class="bold">
                Watch the video below to find out more!
            </p>
            <p class="bold">
                -The Sidewalk Team
            </p>
            <p class="bold">
                PS: All our data and source code is completely open, so we invite you to build your own accessibility applications or perform new analyses with our data. See our <a href="http://projectsidewalk.io/api">API Page</a> and <a href="https://github.com/ProjectSidewalk/SidewalkWebpage">Github Page</a> for details.
            </p>

            <iframe width="560" height="315" src="https://www.youtube.com/embed/_GBLqZDXB_0" frameborder="0" allowfullscreen></iframe>

        </div>
    </div>

    <div class="spacer30"></div>

    <h2 class="">Team Members</h2>
    <div class="row">
        <div class="col-md-4">
            <h3><a href="http://cs.umd.edu/~jonf/">Jon E. Froehlich</a></h3>
            <p>Spring 2012 - Present</p>
            <p>PI, Professor</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://www.cs.umd.edu/~djacobs/">David Jacobs</a></h3>
            <p>Fall 2013 - Present</p>
            <p>Co-PI, Professor</p>
        </div>
        <div class="col-md-4">
            <h3><a href="http://cs.umd.edu/~manaswi/">Manaswi Saha</a></h3>
            <p>Fall 2016 - Present</p>
            <p>Graduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/misaugstad">Michael Saugstad</a></h3>
            <p>Summer 2017 - Present</p>
            <p>Graduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/maddalihanumateja">Teja Maddali</a></h3>
            <p>Summer 2017 - Present</p>
            <p>Graduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/tongning">Anthony Li</a></h3>
            <p>Fall 2016 - Present</p>
            <p>Undergraduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/sbower213">Steven Bower</a></h3>
            <p>Summer 2017 - Present</p>
            <p>Undergraduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/r-holland">Ryan Holland</a></h3>
            <p>Summer 2017 - Present</p>
            <p>High School Intern</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://linkedin.com/in/sarah-smolen-575ab4ab">Sarah Smolen</a></h3>
            <p>Fall 2017 - Present</p>
            <p>Undergraduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/johannkm">Johann Miller</a></h3>
            <p>Fall 2017 - Present</p>
            <p>Undergraduate Student</p>
        </div>
    </div>

    <h2 class="">Alumni</h2>
    <h3 class="">Graduate Student Alumni</h3>
    <div class="row">
        <div class="col-md-4">
            <h3><a href="http://www.kotarohara.com/">Kotaro Hara</a></h3>
            <p>Spring 2012 - Fall 2016</p>
            <p>Graduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/378/">Ladan Najafizadeh</a></h3>
            <p>Summer 2015 - Fall 2016</p>
            <p>Graduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="http://behnezhad.com/">Soheil Behnezhad</a></h3>
            <p>Fall 2016</p>
            <p>Graduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="http://www.cs.umd.edu/~jinsun/">Jin Sun</a></h3>
            <p>Spring 2013 - Fall 2014</p>
            <p>Graduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="http://www.duruofei.com/">Ruofei Du</a></h3>
            <p>Fall 2013</p>
            <p>Graduate Student</p>
        </div>
    </div>

    <h3 class="">Undergraduate Student Alumni</h3>
    <div class="row">
        <div class="col-md-4">
            <h3><a href="https://github.com/adash12">Aditya Dash</a></h3>
            <p>Summer 2017</p>
            <p>Undergraduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/chishankar">Chirag Shankar</a></h3>
            <p>Summer 2017</p>
            <p>Undergraduate Student</p>
        </div>
        <div class="col-md-4">
            <h3><a href="https://github.com/sagec108">Sage Chen</a></h3>
            <p>Summer 2017</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://github.com/m-furman">Maria Furman</a></h3>
            <p>Spring 2017</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://www.linkedin.com/in/jibae/">Ji Hyuk Bae</a></h3>
            <p>Spring 2017</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/428/">Daniil Zadorozhnyy</a></h3>
            <p>Spring 2016 - Fall 2016</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://github.com/zacharylawrence">Zachary Lawrence</a></h3>
            <p>Fall 2013 - Fall 2015</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/432/">Alex Zhang</a></h3>
            <p>Fall 2014 - Fall 2015</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/398/">Christine Chan</a></h3>
            <p>Summer 2015</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/289/">Robert Moore</a></h3>
            <p>Fall 2012 - Fall 2013</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/296/">Sean Panella</a></h3>
            <p>Fall 2012 - Fall 2013</p>
            <p>Undergraduate Student</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/299/">Victoria Le</a></h3>
            <p>Spring 2012 - Spring 2013</p>
            <p>Undergraduate Student</p>
        </div>
    </div>

    <h3 class="">High School Intern Alumni</h3>
    <div class="row">

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/446/">Niles Rogoff</a></h3>
            <p>Summer 2015</p>
            <p>High School Intern</p>
        </div>

        <div class="col-md-4">
            <h3><a href="https://makeabilitylab.umiacs.umd.edu/member/373/">Jonah Chazan</a></h3>
            <p>Summer 2013</p>
            <p>High School Intern</p>
        </div>
    </div>

    <h3 class="">Other Alumni</h3>
    <div class="row">

        <div class="col-md-4">
            <h3><a href="http://stephanienguyen.co/">Stephanie Nguyen</a></h3>
            <p>Spring 2016</p>
            <p>Contributor</p>
        </div>
    </div>

    <div class="spacer30"></div>

    <h2>Sponsors</h2>
    <div class="row">
        <img src='@routes.Assets.at("images/FundingBodies.png")' alt="PDF thumbnail" class="img-responsive pull-left" style="width: 600px;" align="">
    </div>

    <div class="spacer30"></div>

    <h2>Publications</h2>
    <div class="row">
        <table class="table">
            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.pdf")'>
                                The Design of Assistive Location-based Technologies for People with Ambulatory Disabilities: A Formative Study
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Chen, C and Froehlich, J.
                        </dd>
                        <dd>
                            Proceedings of CHI 2016, San Jose, California, USA
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            In this paper, we investigate how people with mobility impairments assess and evaluate accessibility
                            in the built environment and the role of current and emerging location- based technologies therein.
                            We conducted a three-part formative study with 20 mobility impaired participants: a semi-structured
                            interview (Part 1), a participatory design activity (Part 2), and a design probe activity (Part 3).
                            Part 2 and 3 actively engaged our participants in exploring and designing the future of what we call
                            assistive location- based technologies (ALTs)—location-based technologies that specifically incorporate
                            accessibility features to support navigating, searching, and exploring the physical world.
                            Our Part 1 findings highlight how existing mapping tools provide accessibility benefits—even though
                            often not explicitly designed for such uses. Findings in Part 2 and 3 help identify and uncover useful
                            features of future ALTs. In particular, we synthesize 10 key features and 6 key data qualities.
                            We conclude with ALT design recommendations.
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.pdf")'>
                                Characterizing and Visualizing Physical World Accessibility at Scale Using Crowdsourcing, Computer Vision, and Machine Learning
                            </a>
                        </dt>
                        <dd>
                            Hara, K. and Froehlich, J.
                        </dd>
                        <dd>
                            SIGACCESS Newsletter, Issue 113. 2015
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            Imagine a mobile phone application that allows users to indicate their ambulatory ability
                            (e.g., motorized wheelchair, walker) and then receive personalized, interactive accessible route
                            recommendations to their destination. In the following article, Kotaro Hara and Jon Froehlich talk
                            about their research work aimed at developing scalable data collection methods for remotely acquiring street-level
                            accessibility information and novel mobile navigation and map tools.
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.pdf")'>
                                Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Azenkot, S., Campbell, M., Bennett, C., Le, V., Pannella, S., Moore, R., Minckler, K., Ng, R., and Froehlich, J.
                        </dd>
                        <dd>
                            ACM Transactions on Accessibility 2015
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            Low-vision and blind bus riders often rely on known physical landmarks to help locate and
                            verify bus stop locations (e.g., by searching for an expected shelter, bench, or newspaper bin).
                            However, there are currently few, if any, methods to determine this information a priori via
                            computational tools or services. In this article, we introduce and evaluate a new scalable method
                            for collecting bus stop location and landmark descriptions by combining online crowdsourcing and
                            Google Street View (GSV). We conduct and report on three studies: (i) a formative interview study
                            of 18 people with visual impairments to inform the design of our crowdsourcing tool, (ii) a comparative
                            study examining differences between physical bus stop audit data and audits conducted virtually with GSV,
                            and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of
                            crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of
                            landmarks in nonvisual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that
                            minimally trained crowd workers can find and identify bus stop landmarks with 82.5% accuracy across
                            150 bus stop locations (87.3% with simple quality control).
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.pdf")'>
                                Tohme: Detecting Curb Ramps in Google Street View Using Crowdsourcing, Computer Vision, and Machine Learning
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Sun, J., Moore, R., Jacobs, D., and Froehlich, J.
                        </dd>
                        <dd>
                            Proceedings of UIST 2014, Honolulu, Hawaii, USA
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            Building on recent prior work that combines Google Street View (GSV) and crowdsourcing to remotely collect
                            information on physical world accessibility, we present the first “smart” system, Tohme, that combines machine learning,
                            computer vision (CV), and custom crowd interfaces to find curb ramps remotely in GSV scenes.
                            Tohme consists of two workflows, a human labeling pipeline and a CV pipeline with human verification,
                            which are scheduled dynamically based on predicted performance. Using 1,086 GSV scenes (street intersections)
                            from four North American cities and data from 403 crowd workers, we show that Tohme performs similarly in
                            detecting curb ramps compared to a manual labeling approach alone (F- measure: 84% vs. 86% baseline) but at a 13%
                            reduction in time cost. Our work contributes the first CV-based curb ramp detection system, a custom machine-learning
                            based workflow controller, a validation of GSV as a viable curb ramp data source, and a detailed examination of
                            why curb ramp detection is a hard problem along with steps forward.
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.pdf")'>
                                Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Azenkot, S., Campbell, M., Bennett, C., Le, V., Pannella, S., Moore, R., Minckler, K., Ng, R., and
                            Froehlich, J
                        </dd>
                        <dd>
                            Proceedings of ASSETS 2013, Bellevue, Washington, USA
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify
                            bus stop locations (e.g., by searching for a shelter, bench, newspaper bin). However, there are currently few,
                            if any, methods to determine this information a priori via computational tools or services.
                            In this paper, we introduce and evaluate a new scalable method for collecting bus stop location and
                            landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and
                            report on three studies in particular: (i) a formative interview study of 18 people with visual impairments
                            to inform the design of our crowdsourcing tool; (ii) a comparative study examining differences between physical
                            bus stop audit data and audits conducted virtually with GSV; and (iii) an online study of 153 crowd workers on
                            Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV.
                            Our findings reemphasize the importance of landmarks in non-visual navigation, demonstrate that GSV is a viable bus
                            stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5%
                            accuracy across 150 bus stop locations (87.3% with simple quality control).
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.pdf")'>
                                An Initial Study of Automatic Curb Ramp Detection with Crowdsource Verification using Google Street View Images
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Sun, J., Chazan, J., Jacobs, D., and Froehlich, J.
                        </dd>
                        <dd>
                            Poster Proceedings of HCOMP 2013, Palm Springs, California, USA
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            In our previous research, we examined whether minimally trained crowd workers could find,
                            categorize, and assess sidewalk accessibility problems using Google Street View (GSV) images.
                            This poster paper presents a first step towards combining automated methods (e.g., machine vision-based curb ramp detectors)
                            in concert with human computation to improve the overall scalability of our approach.
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.pdf")'>
                                Combining Crowdsourcing and Google Street View to Identify Street-level Accessibility Problems
                            </a>
                        </dt>
                        <dd>
                            Hara, K., Le, V., Froehlich, J.
                        </dd>
                        <dd>
                            Proceedings of CHI 2013, Paris, France
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges;
                            however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori.
                            In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers)
                            to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies:
                            Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users;
                            Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification
                            labels from a total of 402 turkers. We show that turkers are capable of determining the presence of
                            an accessibility problem with 81% accuracy. With simple quality control methods, this number increases to 93%.
                            Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.
                        </dd>
                    </dl>
                </td>
            </tr>

            <tr>
                <td class="col-md-3">
                    <a href='@routes.Assets.at("documents/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.pdf")'>
                        <img src='@routes.Assets.at("images/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                    </a>
                </td>
                <td class="col-md-9">
                    <dl>
                        <dt>
                            <a href='@routes.Assets.at("documents/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.pdf")'>
                                A Feasibility Study of Crowdsourcing and Google Street View to Determine Sidewalk Accessibility

                            </a>
                        </dt>
                        <dd>
                            Hara, K., Le, V., Froehlich, J.
                        </dd>
                        <dd>
                            Poster Proceedings of ASSETS 2012, Boulder, Colorado, USA
                        </dd>
                        <dd>
                            <div class="spacer10"></div>
                            We explore the feasibility of using crowd workers from Amazon Mechanical Turk to identify and
                            rank sidewalk accessibility issues from a manually curated database of 100 Google Street View images.
                            We examine the effect of three different interactive labeling interfaces (Point, Rectangle, and Outline)
                            on task accuracy and duration. We close the paper by discussing limitations and opportunities for future work.
                        </dd>
                    </dl>
                </td>
            </tr>
        </table>
    </div>
</div>
}

