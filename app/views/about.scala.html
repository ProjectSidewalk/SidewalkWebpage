@import models.user.User
@(title: String, user: Option[User] = None)

@main(title) {
    @navbar(user)
    <div class="container">
        <h2 class="">Mission</h2>
        <div class="row">
            <div class="col-lg-12">
                <p class="bold">
                    Our mission is to collect street-level accessibility information from every street in the world and enable
                    design and development of a novel set of location-based technologies for accessibility.
                </p>
                <p>
                    To achieve this, we develop scalable data collection methods to acquire street-level accessibility data
                    with a combination of crowdsourcing, computer vision, and online map imagery&mdash;such as Google Street View.
                </p>
            </div>
        </div>

        <div class="spacer30"></div>

        <h2 class="">Team Members</h2>
        <div class="row">
            <div class="col-md-4">
                <h3><a href="http://cs.umd.edu/~jonf/">Jon E. Froehlich</a></h3>
                <p>Spring 2012 - Present</p>
                <p>PI, Professor</p>
            </div>
            <div class="col-md-4">
                <h3><a href="https://www.cs.umd.edu/~djacobs/">David Jacobs</a></h3>
                <p>Fall 2013 - Present</p>
                <p>Co-PI, Professor</p>
            </div>
            <div class="col-md-4">
                <h3><a href="http://cs.umd.edu/~manaswi/">Manaswi Saha</a></h3>
                <p>Fall 2016 - Present</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Anthony Li</h3>
                <p>Fall 2016 - Present</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Maria Furman</h3>
                <p>Spring 2017 - Present</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Ji Hyuk Bae</h3>
                <p>Spring 2017 - Present</p>
                <p>Undergraduate Student</p>
            </div>

        </div>

        <h2 class="">Alumni</h2>
        <div class="row">
            <div class="col-md-4">
                <h3><a href="http://www.kotarohara.com/">Kotaro Hara</a></h3>
                <p>Spring 2012 - Fall 2016</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3><a href="http://www.duruofei.com/">Ruofei Du</a></h3>
                <p>Fall 2013</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3><a href="http://www.cs.umd.edu/~jinsun/">Jin Sun</a></h3>
                <p>Spring 2013 - Fall 2014</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Ladan Najafizadeh</h3>
                <p>Summer 2015 - Fall 2016</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3><a href="http://behnezhad.com/">Soheil Behnezhad</a></h3>
                <p>Fall 2016</p>
                <p>Graduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Victoria Le</h3>
                <p>Spring 2012 - Spring 2013</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Jonah Chazan</h3>
                <p>Summer 2013</p>
                <p>High School Intern</p>
            </div>
            <div class="col-md-4">
                <h3>Robert Moore</h3>
                <p>Fall 2012 - Fall 2013</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Sean Panella</h3>
                <p>Fall 2012 - Fall 2013</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Zachary Lawrence</h3>
                <p>Fall 2013 - Fall 2015</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Alex Zhang</h3>
                <p>Fall 2014 - Fall 2015</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3>Anthony Li</h3>
                <p>Summer 2015</p>
                <p>High School Intern</p>
            </div>
            <div class="col-md-4">
                <h3>Niles Rogoff</h3>
                <p>Summer 2015</p>
                <p>High School Intern</p>
            </div>
            <div class="col-md-4">
                <h3>Christine Chan</h3>
                <p>Summer 2015</p>
                <p>Undergraduate Student</p>
            </div>
            <div class="col-md-4">
                <h3><a href="http://stephanienguyen.co/">Stephanie Nguyen</a></h3>
                <p>Spring 2016</p>
                <p>Contributor</p>
            </div>
            <div class="col-md-4">
                <h3>Daniil Zadorozhnyy</h3>
                <p>Spring 2016 - Fall 2016</p>
                <p>Undergraduate Student</p>
            </div>
        </div>

        <div class="spacer30"></div>

        <h2>Sponsors</h2>
        <div class="row">
            <img src='@routes.Assets.at("images/FundingBodies.png")' alt="PDF thumbnail" class="img-responsive pull-left" style="width: 600px;" align="">
        </div>

        <div class="spacer30"></div>

        <h2>Publications</h2>
        <div class="row">
            <table class="table">
                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_TheDesignOfAssistiveLocation-basedTechnologiesForPeopleWithAmbulatoryDisabilitiesAFormativeStudy_CHI2016.pdf")'>
                                    The Design of Assistive Location-based Technologies for People with Ambulatory Disabilities: A Formative Study
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Chen, C and Froehlich, J.
                            </dd>
                            <dd>
                                Proceedings of CHI 2016, San Jose, California, USA
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                In this paper, we investigate how people with mobility impairments assess and evaluate accessibility
                                in the built environment and the role of current and emerging location- based technologies therein.
                                We conducted a three-part formative study with 20 mobility impaired participants: a semi-structured
                                interview (Part 1), a participatory design activity (Part 2), and a design probe activity (Part 3).
                                Part 2 and 3 actively engaged our participants in exploring and designing the future of what we call
                                assistive location- based technologies (ALTs)—location-based technologies that specifically incorporate
                                accessibility features to support navigating, searching, and exploring the physical world.
                                Our Part 1 findings highlight how existing mapping tools provide accessibility benefits—even though
                                often not explicitly designed for such uses. Findings in Part 2 and 3 help identify and uncover useful
                                features of future ALTs. In particular, we synthesize 10 key features and 6 key data qualities.
                                We conclude with ALT design recommendations.
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_CharacterizingAndVisualizingPhysicalWorldAccessibilityAtScaleUsingCrowdsourcingComputerVisionAndMachineLearning_SIGACCESSNewsletter2015.pdf")'>
                                    Characterizing and Visualizing Physical World Accessibility at Scale Using Crowdsourcing, Computer Vision, and Machine Learning
                                </a>
                            </dt>
                            <dd>
                                Hara, K. and Froehlich, J.
                            </dd>
                            <dd>
                                SIGACCESS Newsletter, Issue 113. 2015
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                Imagine a mobile phone application that allows users to indicate their ambulatory ability
                                (e.g., motorized wheelchair, walker) and then receive personalized, interactive accessible route
                                recommendations to their destination. In the following article, Kotaro Hara and Jon Froehlich talk
                                about their research work aimed at developing scalable data collection methods for remotely acquiring street-level
                                accessibility information and novel mobile navigation and map tools.
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetViewAnExtendedAnalysis_TACCESS2015.pdf")'>
                                    Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Azenkot, S., Campbell, M., Bennett, C., Le, V., Pannella, S., Moore, R., Minckler, K., Ng, R., and Froehlich, J.
                            </dd>
                            <dd>
                                ACM Transactions on Accessibility 2015
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                Low-vision and blind bus riders often rely on known physical landmarks to help locate and
                                verify bus stop locations (e.g., by searching for an expected shelter, bench, or newspaper bin).
                                However, there are currently few, if any, methods to determine this information a priori via
                                computational tools or services. In this article, we introduce and evaluate a new scalable method
                                for collecting bus stop location and landmark descriptions by combining online crowdsourcing and
                                Google Street View (GSV). We conduct and report on three studies: (i) a formative interview study
                                of 18 people with visual impairments to inform the design of our crowdsourcing tool, (ii) a comparative
                                study examining differences between physical bus stop audit data and audits conducted virtually with GSV,
                                and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of
                                crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of
                                landmarks in nonvisual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that
                                minimally trained crowd workers can find and identify bus stop landmarks with 82.5% accuracy across
                                150 bus stop locations (87.3% with simple quality control).
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_TohmeDetectingCurbRampsInGoogleStreetViewUsingCrowdsourcingComputerVisionAndMachineLearning_UIST2014.pdf")'>
                                    Tohme: Detecting Curb Ramps in Google Street View Using Crowdsourcing, Computer Vision, and Machine Learning
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Sun, J., Moore, R., Jacobs, D., and Froehlich, J.
                            </dd>
                            <dd>
                                Proceedings of UIST 2014, Honolulu, Hawaii, USA
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                Building on recent prior work that combines Google Street View (GSV) and crowdsourcing to remotely collect
                                information on physical world accessibility, we present the first “smart” system, Tohme, that combines machine learning,
                                computer vision (CV), and custom crowd interfaces to find curb ramps remotely in GSV scenes.
                                Tohme consists of two workflows, a human labeling pipeline and a CV pipeline with human verification,
                                which are scheduled dynamically based on predicted performance. Using 1,086 GSV scenes (street intersections)
                                from four North American cities and data from 403 crowd workers, we show that Tohme performs similarly in
                                detecting curb ramps compared to a manual labeling approach alone (F- measure: 84% vs. 86% baseline) but at a 13%
                                reduction in time cost. Our work contributes the first CV-based curb ramp detection system, a custom machine-learning
                                based workflow controller, a validation of GSV as a viable curb ramp data source, and a detailed examination of
                                why curb ramp detection is a hard problem along with steps forward.
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_ImprovingPublicTransitAccessibilityForBlindRidersByCrowdsourcingBusStopLandmarkLocationsWithGoogleStreetView_ASSETS2013.pdf")'>
                                    Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Azenkot, S., Campbell, M., Bennett, C., Le, V., Pannella, S., Moore, R., Minckler, K., Ng, R., and
                                Froehlich, J
                            </dd>
                            <dd>
                                Proceedings of ASSETS 2013, Bellevue, Washington, USA
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify
                                bus stop locations (e.g., by searching for a shelter, bench, newspaper bin). However, there are currently few,
                                if any, methods to determine this information a priori via computational tools or services.
                                In this paper, we introduce and evaluate a new scalable method for collecting bus stop location and
                                landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and
                                report on three studies in particular: (i) a formative interview study of 18 people with visual impairments
                                to inform the design of our crowdsourcing tool; (ii) a comparative study examining differences between physical
                                bus stop audit data and audits conducted virtually with GSV; and (iii) an online study of 153 crowd workers on
                                Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV.
                                Our findings reemphasize the importance of landmarks in non-visual navigation, demonstrate that GSV is a viable bus
                                stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5%
                                accuracy across 150 bus stop locations (87.3% with simple quality control).
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_AnInitialStudyOfAutomaticCurbRampDetectionWithCrowdsourcedVerificationUsingGoogleStreetViewImages_HCOMP2013.pdf")'>
                                    Combining Crowdsourcing and Google Street View to Identify Street-level Accessibility Problems
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Sun, J., Chazan, J., Jacobs, D., and Froehlich, J.
                            </dd>
                            <dd>
                                Poster Proceedings of HCOMP 2013, Palm Springs, California, USA
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                In our previous research, we examined whether minimally trained crowd workers could find,
                                categorize, and assess sidewalk accessibility problems using Google Street View (GSV) images.
                                This poster paper presents a first step towards combining automated methods (e.g., machine vision-based curb ramp detectors)
                                in concert with human computation to improve the overall scalability of our approach.
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_CombiningCrowdsourcingAndGoogleStreetViewToIdentifyStreetLevelAccessibilityProblems_CHI2013.pdf")'>
                                    Combining Crowdsourcing and Google Street View to Identify Street-level Accessibility Problems
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Le, V., Froehlich, J.
                            </dd>
                            <dd>
                                Proceedings of CHI 2013, Paris, France
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges;
                                however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori.
                                In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers)
                                to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies:
                                Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users;
                                Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification
                                labels from a total of 402 turkers. We show that turkers are capable of determining the presence of
                                an accessibility problem with 81% accuracy. With simple quality control methods, this number increases to 93%.
                                Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.
                            </dd>
                        </dl>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3">
                        <a href='@routes.Assets.at("documents/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.pdf")'>
                            <img src='@routes.Assets.at("images/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.png")' alt="PDF thumbnail" class="img-responsive center-block pdf-thumbnail" align="">
                        </a>
                    </td>
                    <td class="col-md-9">
                        <dl>
                            <dt>
                                <a href='@routes.Assets.at("documents/papers/Hara_AFeasibilityStudyOfCrowdsourcingAndGoogleStreetViewToDetermineSidewalkAccessibility_ASSETS2012.pdf")'>
                                    Combining Crowdsourcing and Google Street View to Identify Street-level Accessibility Problems
                                </a>
                            </dt>
                            <dd>
                                Hara, K., Le, V., Froehlich, J.
                            </dd>
                            <dd>
                                Poster Proceedings of ASSETS 2012, Boulder, Colorado, USA
                            </dd>
                            <dd>
                                <div class="spacer10"></div>
                                We explore the feasibility of using crowd workers from Amazon Mechanical Turk to identify and
                                rank sidewalk accessibility issues from a manually curated database of 100 Google Street View images.
                                We examine the effect of three different interactive labeling interfaces (Point, Rectangle, and Outline)
                                on task accuracy and duration. We close the paper by discussing limitations and opportunities for future work.
                            </dd>
                        </dl>
                    </td>
                </tr>
            </table>
        </div>
    </div>
}